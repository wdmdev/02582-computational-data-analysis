{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'toolbox.error' from '/Users/williammarstrand/privat/DTU/Kandidat/1. Semester/Computational Data Analysis/02582-computational-data-analysis/week3/../toolbox/error.py'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn import linear_model \n",
    "from scipy import linalg\n",
    "from sklearn import preprocessing as preproc # load preprocessing function\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.colors as colors\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# seaborn can be used to \"prettify\" default matplotlib plots by importing and setting as default\n",
    "import seaborn as sns\n",
    "sns.set() # Set searborn as default\n",
    "\n",
    "import imp\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join('..'))\n",
    "from toolbox import error\n",
    "imp.reload(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat('sand.mat')\n",
    "\n",
    "X = mat['X']\n",
    "y = mat['Y']\n",
    "\n",
    "[n,p] = X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Apply Least angle regression and selection (LARS) for the p >> n sand data set (X: data matrix with 59 observations and 2016 features, y: the measured moisture content in percent for each sand sample). Find a suitable solution using:\n",
    "\n",
    ">(a) The Cp statistic. Consider whether the Cp-statistic makes sense in this case (p > n). Why? Why not?\n",
    "\n",
    ">> (i) Hint: What happens to your estimate of the noise in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.145311480746789e-24"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estimate the noise of the data\n",
    "# Calculate an unbiased OLS estimate using linalg.lstsq\n",
    "Betas = linalg.lstsq(X,y)[0]\n",
    "error.compute_mse(X, y, Betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">(b) Using Cross-validation. Remember to center y and normalize X, but do it inside\n",
    "the cross validation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centerData(data):\n",
    "    \n",
    "    mu = np.mean(data,axis=0)\n",
    "    data = data - mu\n",
    "    \n",
    "    return data, mu\n",
    "\n",
    "# Set up cross validation like you did last week\n",
    "CV = 5 # if K = n leave-one-out, you may try different numbers\n",
    "kf = KFold(n_splits=CV)\n",
    "\n",
    "# Set a suitable range of features that can be given to LARS as n_nonzero_coefs \n",
    "stop = n-math.ceil(n/CV)\n",
    "K = range(stop)\n",
    "\n",
    "Err_tr = np.zeros((CV,len(K)))\n",
    "Err_tst = np.zeros((CV, len(K))) \n",
    "for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    # NOTE: If you normalize outside the CV loop the data implicitly carry information of the test data\n",
    "    # We should perform CV \"the right way\" and keep test data unseen.\n",
    "\n",
    "\n",
    "    # compute all LARS solutions inside each fold \n",
    "\n",
    "        # Predict with this model, and find both training and test error\n",
    "\n",
    "\n",
    "\n",
    "err_tr = np.mean(Err_tr, axis=0) # mean training error over the CV folds\n",
    "err_tst = np.mean(Err_tst, axis=0) # mean test error over the CV folds\n",
    "err_ste = np.std(Err_tst, axis=0)/np.sqrt(CV) # Note: we divide with sqrt(n) to get the standard error as opposed to the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Cp-statistic, assumption n > p\n",
    "\n",
    "# run LARS on all data and and vary the nonzero coefs from 0 to p. save the betas for Cp\n",
    "\n",
    "# calculate Cp for each number of nonzero coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(15,5))    \n",
    "ax[0].plot(K, err_tr, 'b', label='train')\n",
    "ax[0].plot(K, err_tst, 'r', label='test')\n",
    "ax[0].plot(K, Cp/1e27, 'g', label= 'C_p') # scale to put in same plot\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel('k')\n",
    "ax[0].set_ylabel('error estimate')\n",
    "ax[0].set_title(\"error estimate\")\n",
    "\n",
    "ax[1].plot(K, np.log(err_tr), 'b', label='train')\n",
    "ax[1].plot(K, np.log(err_tst), 'r', label='test')\n",
    "ax[1].plot(K, Cp/1e27, 'g', label= 'C_p') # scale to put in same plot\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel('k')\n",
    "ax[1].set_ylabel('error estimate')\n",
    "ax[1].set_title(\"Log error estimate\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
