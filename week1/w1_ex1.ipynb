{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To embed plots in the notebooks\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np # numpy library\n",
    "import scipy . linalg as lng # linear algebra from scipy library\n",
    "from scipy . spatial import distance # load distance function\n",
    "from sklearn import preprocessing as preproc # load preprocessing function\n",
    "\n",
    "# seaborn can be used to \"prettify\" default matplotlib plots by importing and setting as default\n",
    "import seaborn as sns\n",
    "sns.set() # Set searborn as default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetPath = './DiabetesDataNormalized.txt'\n",
    "T = np.loadtxt(diabetPath, delimiter = ' ', skiprows = 1)\n",
    "y = T[:, 10]\n",
    "X = T[:,:10]\n",
    "\n",
    "# Get number of observations (n) and number of independent variables (p)\n",
    "[n, p] = np.shape(X)\n",
    "\n",
    "M = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Solve the Ordinary Least Squares (OLS) computationally (for the diabetes data set):\n",
    "\n",
    "> (a) What is the difference between using a brute force implementation(analytical) for an OLS solver and a numerically ’smarter’ implementation? Compute the ordinary least squares solution to the diabetes data set for both options and look at the relative difference. Use for example lng.lstsq to invert the matrix or to solve the linear system of equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols_numerical(X, y):\n",
    "    return np.linalg.lstsq(X,y)\n",
    "\n",
    "def ols_analytical(X, y):\n",
    "    # Implement the analytical closed form way of calculating the betas \n",
    "    return np.linalg.inv(X.T@X)@(X.T@y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The list of betas: \n",
      "[-0.00618293 -0.14813008  0.32110005  0.20036692 -0.48931352  0.29447365\n",
      "  0.06241272  0.10936897  0.46404908  0.04177187]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bb/6zx8vn690fd_kml8spjg6j8h0000gn/T/ipykernel_13128/966349877.py:2: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  return np.linalg.lstsq(X,y)\n"
     ]
    }
   ],
   "source": [
    "# numerical solution\n",
    "beta_num, _, _, _ = ols_numerical(M, y)\n",
    "print(f'The list of betas: \\n{beta_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The list of betas: \n",
      "[-0.00618293 -0.14813008  0.32110005  0.20036692 -0.48931352  0.29447365\n",
      "  0.06241272  0.10936897  0.46404908  0.04177187]\n"
     ]
    }
   ],
   "source": [
    "# analytical solution\n",
    "beta_ana = ols_analytical(M,y)\n",
    "print(f'The list of betas: \\n{beta_ana}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of the difference between betas: \n",
      "1.3509436705606625e-14\n"
     ]
    }
   ],
   "source": [
    "# difference in solutions\n",
    "norm = np.linalg.norm(beta_ana-beta_num)\n",
    "print(f'The norm of the difference between betas: \\n{norm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the difference significant? \n",
    "\n",
    "What can we conclude relating to numerical vs analytical solutions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (b) How could you include an intercept term in Python? This means using the model: $y = β_0 +xβ_1 +...+x_pβ_p +ε $ rather than: $ y=x_1β_1 +...+x_pβ_p +ε. $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical B1: -0.006117424418889521\n",
      "Analytical B1: -0.00611742441889071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bb/6zx8vn690fd_kml8spjg6j8h0000gn/T/ipykernel_13128/966349877.py:2: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  return np.linalg.lstsq(X,y)\n"
     ]
    }
   ],
   "source": [
    "# Include offset / intercept\n",
    "MM = np.vstack((np.ones(X.shape[1]), M))\n",
    "yy = np.insert(y, 0, 1)\n",
    "\n",
    "beta_num_2, _, _, _ = ols_numerical(MM, yy)\n",
    "beta_ana_2 = ols_analytical(MM,yy)\n",
    "\n",
    "print(f'Numerical B1: {beta_num_2[0]}')\n",
    "print(f'Analytical B1: {beta_ana_2[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the value of the intercept coefficient?\n",
    "\n",
    "Can you explain why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (c) Calculate the mean squared error $MSE = 1/n \\sum^n_{i=1} (y_i−x_iβ)^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the estimated y values and use these to calculate the MSE.\n",
    "def compute_mse(X,beta):\n",
    "    n = X.shape[0]\n",
    "    return 1/n * np.sum((y-X@beta)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse from the analytical solution: 0.48116051086159695\n"
     ]
    }
   ],
   "source": [
    "# , res_ana, yhat_ana\n",
    "mse_ana = compute_mse(M,beta_ana)\n",
    "\n",
    "print(f'mse from the analytical solution: {mse_ana}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens to the MSE if we change some of the betas?\n",
    "\n",
    "Is that what you expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse from the changed betas: 0.56767905201326\n"
     ]
    }
   ],
   "source": [
    "beta_new = beta_ana.copy()\n",
    "beta_new[5] = 0\n",
    "\n",
    "# , res_new, yhat_new\n",
    "mse_new = compute_mse(M,beta_new)\n",
    "\n",
    "print(f'mse from the changed betas: {mse_new}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (d) Calculate the residual sum of squares $RSS = ∥{\\bf y} − Xβ∥_2^2$ and the total sum of squares $TSS = ∥{\\bf y} − y ̄∥_2^2$, where $y ̄$ is the estimated mean of ${\\bf y}$. Report on the $R^2$ measure, that is, the proportion of variance in the sample set explained by the\n",
    "  model: $R^2 = 1 − \\frac{RSS}{TSS}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSS: 212.67294580082586\n",
      "TSS: 441.0000000000003\n",
      "R^2: 0.5177484222203499\n"
     ]
    }
   ],
   "source": [
    "RSS = np.linalg.norm(y-X@beta_ana)**2\n",
    "TSS = np.linalg.norm(y-np.mean(y))**2\n",
    "R_sq = 1 - RSS/TSS\n",
    "\n",
    "print(f'RSS: {RSS}')\n",
    "print(f'TSS: {TSS}')\n",
    "print(f'R^2: {R_sq}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much variance in <strong>y</strong> can we explain using this simple model?"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
